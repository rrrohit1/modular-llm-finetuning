{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06578772",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma-3-1b on GSM8K Dataset\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Gemma-3-1b model using the GSM8K dataset with LoRA (Low-Rank Adaptation) for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -qqq \"transformers>=4.55.0\" \"trl>=0.22.1\" \"datasets\" \"torch\"\n",
    "!pip install -qqq \"accelerate\" \"peft\" \"huggingface_hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17138fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemma-3-1b base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format the GSM8K dataset\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "def format_to_messages(example):\n",
    "    system_instruction = \"\"\"You are a highly logical and analytical problem-solving engine. When presented with a complex math word problem, your primary objective is to generate a comprehensive, step-by-step thinking process. Each step must clearly state the calculation performed and the resulting intermediate value. Ensure the final answer is extracted from the solution steps and is the last output.\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_instruction},\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"answer\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "gsm8k_formatted = dataset.map(format_to_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965769af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA and SFT configurations\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\",\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./gemma-lora-demo\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    packing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38caf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=gsm8k_formatted,\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
